
This document describes how to run ensembl-compara ProteinTrees pipeline.

1. Necessary software components:

	* MySQL 5.1          (or higher)
	* Perl 5.14          (or higher)
	* Perl DBI API 1.6   (or higher)

EnsEMBL and BioPerl software:
	* bioperl-live (version 1.6.9)               - Base of the BioPerl distribution. Mainly needed to provide I/O for alignments
	* bioperl-run (version 1.6.9)                - Needed for the CodeML runnable/parser
	* ensembl ("branch-ensembl-74" tag)          - Core API on which the rest of ensembl APIs are based
	* ensembl-compara ("branch-ensembl-74" tag)  - Compara API (data objects, db adaptors, pipeline runnables, pipeline configuration)
	* ensembl-hive ("stable" tag)                - The system to run pipelines
!! Please ensure that your PERL5LIB includes all of these modules

Refer to the following pages for tips about installation and setting up the environment:
	http://www.ensembl.org/info/docs/api/api_installation.html
	http://www.ensembl.org/info/docs/eHive/installation.html

Perl libraries:

	 (mandatory)
	* Statistics::Descriptive    - Used during the dN/dS computation
	* Parse::RecDescent          - To export trees in newick

	 (optional)
	* JSON                       - Used to configure the non-Ensembl species on which the pipeline has to run
	* FamLibBuilder              - Only needed for the HMM-based clustering. Part of the Panther distribution
	* XML::Writer                - Used to output trees in OrthoXML and PhyloXML


Any compiled binaries mentioned in ensembl-compara/modules/Bio/EnsEMBL/Compara/PipeConfig/Example/EnsemblProteinTrees_conf.pm
Here is the list of the versions that we used for the e74 production:

	* NCBI-blast 2.2.2+   - ftp://ftp.ncbi.nlm.nih.gov/blast/executables/blast+/LATEST/
	* mcoffee 9.03.r1318   - http://www.tcoffee.org/Projects/mcoffee/
	* MAFFT 7.113          - http://mafft.cbrc.jp/alignment/software/
	* hcluster_sg          - http://treesoft.svn.sourceforge.net/viewvc/treesoft/branches/lh3/hcluster/
	* treebest             - https://github.com/muffato/treebest
	* quicktree 1.1        - http://www.sanger.ac.uk/resources/software/quicktree/
	* hmmbuild 3.0         - (part of the HMMER package) http://hmmer.janelia.org/
	* codeml 4.3           - (part of the PAML package) http://abacus.gene.ucl.ac.uk/software/paml.html WARNING: The pipeline does not support more recent versions
	* Ktreedist 1.0        - http://molevol.cmima.csic.es/castresana/Ktreedist.html


The pipeline can connect to a "master" database to initialize. Please refer to the file "README-master_database" to correctly
set it up. You will have to import all the genome_dbs (the species on which you want to run the pipeline) with:
 - ensembl-compara/scripts/pipeline/update_genome.pl for the Ensembl / Ensembl Genomes species
 - a manual SQL INSERT. Currently, the species that are defined in the JSON file have to be also defined in the master database manually
 - INSERT INTO genome_db (taxon_id, name, assembly, assembly_default, genebuild) VALUES ( ... /the same values as in the JSON file/ ...);


2. General structure of the pipeline

You can refer to docs/pipeline_diagrams/ProteinTrees.png for a visual description of the pipeline.

The main structure is given by some backbone analysis. Each one of them will dump the current state of the database (for a backup) and 
fire the next step of the pipeline. The pipeline also contains numerous health-check analysis (named hc_*) that should detect as early as possible any error.
The pipeline will follow one of the two paths (A or B). 'A' is a clustering based on all-vs-all blastp. 'B' is a HMM-based clustering.
The option is selected by the hmm_clustering flag.

   2.1. db_prepare

At this step, the pipeline will initialize:
 - the ncbi_taxa_node and ncbi_taxa_name tables: copied over from a reference database (either a "master" database, or a pre-existing Compara database)
 - entries in the method_link, species_set, and method_link_species_set tables

Then, it will:
 - check that the connections to each core database / FASTA file are available
 - check whether some species-specific data can be reused from a reference Compara database (to save some time at the later stages of the pipeline). This is only available if you are running the pipeline with a master database.
 - build the default species tree (using the NCBI taxonomy)

   2.2. genome_load

At this step, the pipeline will actually load all the data related to the species:
 - the list of members (genes and peptides)
 - the peptide sequences
 - the list of canonical transcripts (in case of alternative splicing: the isoform / sequence that should be used in the pipeline)

   2.3. (path A) allvsallblast

At this step, the pipeline will run the all-vs-all blastp comparisons. Some hits can be "reused" from the reference compara database, which
can save quite some time of computation.

   2.4. (path A) hcluster

At this step, the pipeline will build a graph from the blast hits, and run hcluster_sg on it. The resulting clusters contain similar genes and will
map to individual gene-trees.

   2.5. (path B) hmmClassify

At this step, the pipeline will load all the HMM profiles defined in the library, and classify all the genes from all the species into them.
Each profile will naturally define a cluster.

   2.6. tree_building

At this step, the pipeline will actually compute the trees with the
 - multiple alignment (Mcoffee if the cluster has less than 250 genes, Mafft otherwise)
 - tree reconstruction with TreeBest
 - homology inference

To prevent computation issues, the largest clusters (more than 400 genes) are recursively split in halves until they fall until the limit size
with the QuickTree program (using a Mafft alignment)

   2.7. dnds

At this step, the pipeline will compute dN/dS values on all the homologies (this can be parameterized)

   2.8 namemapping


3. Pipeline configuration

The pipeline structure (analysis work-flow) is defined in ensembl-compara/modules/Bio/EnsEMBL/Compara/PipeConfig/ProteinTrees_conf.pm but the actual
parameters used by the various groups at the Genome Campus are defined in ensembl-compara/modules/Bio/EnsEMBL/Compara/PipeConfig/Example/*ProteinTrees_conf.pm
They mainly include custom:
 - paths to executables
 - database connection parameters
 - more general parameters (pipeline-related)
 - beekeeper parameters

To configure the pipeline, you can:
 - either edit PipeConfig/ProteinTrees_conf.pm (uncomment and update the commented parameters)
 - or copy and update one of the example configuration files

Here follows a description of each category of parameters

   3.1. Path to executables

As stated in the first section of this document, the pipeline relies on some external programs to do the computation.
Make sure that all the necessary software are installed and properly configured.
All the *_exe parameters must point to their correct locations

   3.2 Database connections

The configuration file must define 'pipeline_db': the database to hold the data.

If you are running the pipeline with a master database, define its connection parameters in 'master_db', and set the 'use_master_db' flag to 1
Otherwise, define the 'ncbi_db' database and set the 'use_master_db' flag to 0

When using a master database, make sure that that all the SpeciesSet and MethodLinkSpeciesSet entries have been populated for the species you want to run the pipeline on.

The pipeline relies on some Ensembl core (species) databases to provide the species-specific data. This can be configured with the 'curr_core_sources_locs'
parameter, which is a list of database connections. It should contain the same server list as you have used when running scripts/pipeline/update_genome.pl

If you are going to use Ensembl data, you may want to add the following database description:
'ensembl_srv' => {
	-host   => 'ensembldb.ensembl.org',
	-port   => 5306,
	-user   => 'anonymous',
	-pass   => '',
},
'curr_core_sources_locs' => [ $self->o('ensembl_srv') ],

If you are going to run the pipeline on species that are not in Ensembl, you have to define the "curr_file_sources_locs" parameter with a JSON formatted file.
The JSON file needs to contain meta data for each species and should have the following format:

[
{
        "production_name" : "nomascus_leucogenys",
        "taxonomy_id"     : "61853",
        "assembly"        : "Nleu2.0",
        "genebuild"       : "2011-05",
        "prot_fasta"      : "proteins.fasta",
        "cds_fasta"       : "transcripts.fasta",
        "gene_coord_gff"  : "annotation.gff",
}
]

If you want to use a Compara database as a reference (for example, to reuse the results of the all-vs-all blastp), you have to set the 'reuse_from_prev_rel_db' flag on, and configure the 'reuse_db' parameter: 
'prev_rel_db' => {
	-host   => 'ensembldb.ensembl.org',
	-port   => 5306,
	-user   => 'anonymous',
	-pass   => '',
	-dbname => 'ensembl_compara_XXXX',
},
Then, you will have to update the 'prev_core_sources_locs' parameter. It is equivalent to 'curr_core_sources_locs', but refers to the core databases
linked to 'reuse_db'. Again, on Ensembl data, you can define: 'prev_core_sources_locs' => [ $self->o('ensembl_srv') ] 
Please make sure that 'prev_release' contains the version number of the reuse database.

   3.3. More general parameters (pipeline-related)

 - mlss_id: the method_link_species_set_id created by scripts/pipeline/create_mlss.pl
   This defines the instance of the pipeline (which species to work on). It is only needed if you run the pipeline with a master database. Otherwise, the pipeline will create its own one.

   To get it from the master database, run the following query:
   > SELECT * FROM method_link_species_set WHERE method_link_id = 401;
   You can check the content of a species_set_id XXX this way:
   > SELECT name FROM species_set JOIN genome_db USING (genome_db_id) WHERE species_set_id = XXX ORDER BY name;

 - release: the API version of your CVS checkout

 - rel_suffix: any string (defaults to "") to distinguish between several runs on the same API version

 - work_dir: where to store temporary files
   The pipeline will create there 3 folders:
    - blast_db: the blast databases for the all-vs-all blastp
    - cluster: files used by hcluster_sg
    - dumps: SQL dumps (checkpoints) of the database

 - outgroups: the list of outgroup species (genome_db names)
   This is used by hcluster_sg to produce more relevant clusters. It allows two levels of outgroups (named as "2" and "4", "4" being the most out)
   In the Ensembl run, we only define S.cerevisae as outgroup (level 2). Hence the configuration: {'saccharomyces_cerevisiae' => 2}

 - taxlevels: on which clades should the pipeline try to compute dN/dS values.
   Those values are only available for close enough species and it is generally not a good idea to use very large clades (like the animal kingdom.
   The parameter is a list of taxa (given by their names in the NCBI taxonomy). The list can be empty to skip this step of the pipeline.
   In Ensembl, we only use mammals, some birds and some fish, in the config file this is shown as ['Theria', 'Sauria', 'Tetraodontiformes']

   3.4. beekeeper parameters

All the *_capacity parameters are tuned to fit the capacity of our MySQL servers. You might want to initially reduce them, and gradually increase
them "as long as the database holds" :) The relative proportion of each analysis should probably stay the same

The "resource_classes" of the configuration file defined how beekeeper should run each category of job. These are LSF parameters that you may only
want to change if you don't have a LSF installation

4. Run the pipeline

The pipeline is now ready to be run.
You can switch to the file "README-beekeeper", which explains how to run beekeeper :)



